{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd3bda44-b63f-45db-a401-f0e56210c85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import mnist, fashion_mnist, cifar10\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Reshape, Concatenate\n",
    "from keras.layers import Conv2D, LeakyReLU, Dropout, Flatten, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import ones, zeros, expand_dims, vstack, asarray\n",
    "from numpy import linspace, arccos, clip, sin, cos, dot\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import rand, randn, randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0559c-1b68-4593-a896-4ce83bbbeb58",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7098d370-7a3f-4e84-bda0-a50172132981",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path.cwd()\n",
    "\n",
    "OPT = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "DATASET_INFO = {\n",
    "    'mnist': {\n",
    "        'class_names': [str(i) for i in range(10)],\n",
    "        'input_shape': (28, 28, 1)\n",
    "    },\n",
    "    'fashion_mnist': {\n",
    "        'class_names': ['t-shirt/top', 'trouser', 'pullover', 'dress', 'coat', \n",
    "                        'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot'],\n",
    "        'input_shape': (28, 28, 1)\n",
    "    },\n",
    "    'cifar10':{\n",
    "        'class_names': ['airplane', 'car', 'bird', 'cat', 'deer', \n",
    "                        'dog', 'frog', 'horse', 'ship', 'truck'],\n",
    "        'input_shape': (32, 32, 3)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6fa5d6b-bf82-4394-8b88-224846166978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data=None, dataset_name='mnist', n=5, \n",
    "              save=False, show=False,\n",
    "              name=None, path='/plot_data/',\n",
    "              fontsize=14, axis='off', cmap='gray_r'):\n",
    "\n",
    "    images, labels = data\n",
    "    figsize = (n*2, n*2)\n",
    "    plt.figure(figsize=(figsize))\n",
    "    for i in range(n**2):\n",
    "        plt.subplot(n, n, i+1)\n",
    "        plt.axis(axis)\n",
    "        plt.imshow(images[i].squeeze(), cmap=cmap)\n",
    "        if labels is not None:\n",
    "            plt.title(DATASET_INFO[dataset_name]['class_names'][labels[i].squeeze()], fontsize=fontsize)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        while name is None:\n",
    "            name = input(\"Enter name for figure: \")\n",
    "\n",
    "        file_path = cwd / Path('figures') / Path(path)\n",
    "        file_path.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(file_path.joinpath(str(name) + '.png'))\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3499b8a-f7c4-4bf9-833b-938a825e873a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, test = mnist.load_data()\n",
    "plot_data(data=train, show=True, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c962d-44c6-4e32-ad2e-e53310440fe0",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28773d57-c826-4719-9a41-47c627cd1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(dataset_name='mnist', \n",
    "                         n_classes=10, opt=OPT):\n",
    "\n",
    "    in_shape = DATASET_INFO[dataset_name]['input_shape']\n",
    "    \n",
    "    # LABEL INPUT\n",
    "    in_label = Input(shape=(1,))\n",
    "    \n",
    "    # EMBEDDING FOR CATEGORICAL INPUT\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "\n",
    "    n_nodes = in_shape[0] * in_shape[1] * 1\n",
    "    li = Dense(n_nodes)(li)\n",
    "\n",
    "    # RESHAPING TO MATCH THE DIMENSIONS OF THE IMAGE\n",
    "    li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "    \n",
    "    # INPUT OF THE IMAGE\n",
    "    in_image = Input(shape=in_shape)\n",
    "    \n",
    "    # CONCATENATE LABEL TO THE CHANNELS\n",
    "    merge = Concatenate()([in_image, li])\n",
    "\n",
    "    # DOWNSAMPLE 1\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    if in_shape[2] == 3: assert fe.shape == (None, 16, 16, 128)\n",
    "    if in_shape[2] == 1: assert fe.shape == (None, 14, 14, 128)\n",
    "    \n",
    "    # DOWNSAMPLE 2\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)    \n",
    "    if in_shape[2] == 3: assert fe.shape == (None, 8, 8, 128)\n",
    "    if in_shape[2] == 1: assert fe.shape == (None, 7, 7, 128)\n",
    "\n",
    "    # DOWNSAMPLE 3\n",
    "    if in_shape[2] == 3:\n",
    "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)    \n",
    "        assert fe.shape == (None, 4, 4, 128)\n",
    "    \n",
    "    # FLATTEN\n",
    "    fe = Flatten()(fe)\n",
    "    # DROPOUT\n",
    "    fe = Dropout(0.4)(fe)\n",
    "    # OUTPUT\n",
    "    out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "    \n",
    "    model = Model([in_image, in_label], out_layer)                            \n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=opt, \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de1a1f25-9c61-44e2-a811-db15fde81529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(dataset_name='mnist',\n",
    "                     latent_dim=100, \n",
    "                     n_classes=10):\n",
    "\n",
    "    input_shape = DATASET_INFO[dataset_name]['input_shape']\n",
    "    in_label = Input(shape=(1,))\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "\n",
    "    node = 7 \n",
    "    if input_shape[2] == 3:\n",
    "        node = 4\n",
    "\n",
    "    n_nodes = node * node\n",
    "\n",
    "    li = Dense(n_nodes)(li)\n",
    "    li = Reshape((node, node, 1))(li)\n",
    "\n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "    \n",
    "    # FOUNDATION FOR THE IMAGE\n",
    "    n_nodes = 128 * node * node\n",
    "    gen = Dense(n_nodes)(in_lat)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = Reshape((node, node, 128))(gen)\n",
    "    # merge image gen and label input\n",
    "    merge = Concatenate()([gen, li])\n",
    "\n",
    "    # UPSAMPLE 1\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    if input_shape[2] == 1: assert gen.shape == (None, 14, 14, 128)\n",
    "    if input_shape[2] == 3: assert gen.shape == (None, 8, 8, 128)\n",
    "    \n",
    "    # UPSAMPLE 2\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    if input_shape[2] == 1: assert gen.shape == (None, 28, 28, 128)\n",
    "    if input_shape[2] == 3: assert gen.shape == (None, 16, 16, 128)\n",
    "\n",
    "    if input_shape[2] == 3:\n",
    "        gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        if input_shape[2] == 3: assert gen.shape == (None, 32, 32, 128)\n",
    "    \n",
    "    # OUTPUT\n",
    "    out_layer = Conv2D(input_shape[2], (7,7), activation='tanh', padding='same')(gen)\n",
    "    assert out_layer.shape == (None, input_shape[0], input_shape[1], input_shape[2])\n",
    "    \n",
    "    model = Model([in_lat, in_label], out_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbae1855-9f5b-4d4c-b0ef-edf9d9d3721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model ,opt=OPT):\n",
    "    d_model.trainable = False\n",
    "    \n",
    "    # GET NOISE AND LABELS FROM GENERATOR\n",
    "    gen_noise, gen_label = g_model.input\n",
    "    \n",
    "    # IMAGE OUTPUT FROM GENERATOR\n",
    "    gen_output = g_model.output\n",
    "    \n",
    "    # CONNECTING IMAGE OUTPUT AND LABEL INPUT FROM THE GENERATOR -> DISCRIMINATOR\n",
    "    gan_output = d_model([gen_output, gen_label])\n",
    "    \n",
    "    # GAN MODEL -> TAKING IN NOISE AND LABEL AND OUTPUTS A CLASSIFICATION\n",
    "    model = Model([gen_noise, gen_label], gan_output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23de85c-5966-4a8a-8c7e-bce130070259",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b753375a-08d1-42fe-8c51-b4bc42bf7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples(dataset_name='mnist', \n",
    "                      dataset=mnist):\n",
    "    input_shape = DATASET_INFO[dataset_name]['input_shape']\n",
    "    \n",
    "    # Load the MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = dataset.load_data()\n",
    "    \n",
    "    # Reshape and normalize the input data\n",
    "    x_train = (x_train.reshape(x_train.shape[0], input_shape[0], input_shape[1], input_shape[2]).astype('float32') - 127.5) / 127.5\n",
    "                          \n",
    "    # Define the augmentation parameters\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=25,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    # Generate augmented images and labels\n",
    "    augmented_data = datagen.flow(x_train, y_train, batch_size=len(x_train), seed=42)\n",
    "    augmented_images, augmented_labels = augmented_data.next()\n",
    "    \n",
    "    # Append the augmented data to the original dataset\n",
    "    x_train_augmented = np.concatenate([x_train, augmented_images])\n",
    "    y_train_augmented = np.concatenate([y_train, augmented_labels])\n",
    "    return x_train_augmented, y_train_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5ac6bbc-9051-4d75-8ba0-cbe8fac1ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "    # choose random instances\n",
    "    ix = randint(0, images.shape[0], n_samples)\n",
    "    # select images and labels\n",
    "    X, labels = images[ix], labels[ix]\n",
    "    # generate class labels\n",
    "    y = ones((n_samples, 1))\n",
    "    return [X, labels], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3be2364-8627-49bb-848d-a8ef80bff1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples, n_classes=10):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    # generate labels\n",
    "    labels = randint(0, n_classes, n_samples)\n",
    "    return [z_input, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0296940-61dd-4ca7-a538-192be919f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    images = generator.predict([z_input, labels_input])\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return [images, labels_input], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a876105-a314-4b6f-bf18-16c88fd79cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g_model = define_generator()\n",
    "\n",
    "(X, y) = dataset\n",
    "# X, y = generate_latent_points(latent_dim=100, n_samples=1, n_classes=10)\n",
    "# X = g_model.predict([X, y])\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(X[i], cmap='gray_r')\n",
    "    plt.title(y[i])\n",
    "    plt.axis('off')\n",
    "plt.savefig('training_data.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c9e1d2-5890-4bfb-97d8-bc3f9aed4647",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd769660-298f-4c79-8489-a80d6aef8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(g_model, epoch, path=None):\n",
    "    file_path = cwd / Path('models') / Path(path)\n",
    "    file_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    g_model.save(file_path / f'gen_model_e-{epoch+1:03d}.h5')\n",
    "    \n",
    "def summarise_performance(epoch, g_model, latent_dim, n_samples=100):\n",
    "    [X, labels], y = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    X = (X + 1) / 2.0\n",
    "    \n",
    "    plot_data(data=(X, labels), dataset_name='fashion_mnist', save=True, \n",
    "              axis='off', path=f'{GAN_NAME}/images', \n",
    "              name=f'gen_image_e-{epoch+1:03d}', show=True)\n",
    "    \n",
    "    save_model(g_model, epoch, path=f'{GAN_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cb1351d-d717-440a-8457-058a0d714fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128):\n",
    "    bat_per_epoch = dataset[0].shape[0] // n_batch\n",
    "    half_batch = n_batch // 2\n",
    "    hash = {\n",
    "        'd_loss1': [],\n",
    "        'd_loss2': [],\n",
    "        'g_loss': []\n",
    "    }\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(bat_per_epoch):\n",
    "            [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
    "            d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
    "            hash['d_loss1'].append(d_loss1)\n",
    "            \n",
    "            [X_fake, labels_fake], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            d_loss2, _ = d_model.train_on_batch([X_fake, labels_fake], y_fake)\n",
    "            hash['d_loss2'].append(d_loss2)\n",
    "            \n",
    "            [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            \n",
    "            g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
    "            hash['g_loss'].append(g_loss)\n",
    "\n",
    "            print(f\">[{i+1:03d}/{n_epochs}] | Batch: [{j+1:03d}/{bat_per_epoch:03d}] | D-Real: {d_loss1:.3f} | D-Fake: {d_loss2:.3f} | G: {g_loss:.3f}\")\n",
    "\n",
    "            if j % 30 == 0:\n",
    "                clear_output(wait=True)\n",
    "        summarise_performance(i, g_model, latent_dim)\n",
    "    return hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0851002c-2bac-4c96-9680-15ab601710de",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "d_model = define_discriminator(dataset_name='fashion_mnist')\n",
    "g_model = define_generator(latent_dim=latent_dim, dataset_name='fashion_mnist')\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "\n",
    "GAN_NAME = f'cgan/fashion_mnist'\n",
    "dataset = load_real_samples(dataset=fashion_mnist, dataset_name='fashion_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b52e0f-1cfc-44f0-a368-798f3d126bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "hash = train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=200, n_batch=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451b001-0582-437d-982d-5e6af777c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame.from_dict(hash).to_csv('fashion_mnist.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ade2fd-766a-490b-9579-21b9b9edff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5020db-68c7-4d70-845a-a062bb5545a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
