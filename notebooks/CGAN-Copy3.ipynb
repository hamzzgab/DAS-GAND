{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd3bda44-b63f-45db-a401-f0e56210c85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import mnist, fashion_mnist, cifar10\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Reshape, Concatenate\n",
    "from keras.layers import Conv2D, LeakyReLU, Dropout, Flatten, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import ones, zeros, expand_dims, vstack, asarray\n",
    "from numpy import linspace, arccos, clip, sin, cos, dot\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import rand, randn, randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5bccc3-75a4-4612-a3d5-34b05b6aa0ba",
   "metadata": {},
   "source": [
    "# Create the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a00b654c-c1ca-411c-a984-92b151b3d792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cda66d0f4e4f14a5cdef3b80bda6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872e9d1a41ae4ab2b3508cab119cfad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigDataPath = Path('/Users/hamzz/Development/bigData/GTSRB')\n",
    "\n",
    "trainPath = bigDataPath / Path('Train')\n",
    "testPath = bigDataPath / Path('Test')\n",
    "\n",
    "X_train, y_train = [], []\n",
    "X_test, y_test = [], []\n",
    "\n",
    "for file in tqdm(sorted(trainPath.glob('**/*.png'))):\n",
    "    if file.parent.name != '.ipynb_checkpoints':\n",
    "        image = cv2.imread(str(file))\n",
    "        image = cv2.resize(image, (32, 32))\n",
    "        X_train.append(image)\n",
    "        y_train.append(int(file.parent.name))\n",
    "\n",
    "df = pd.read_csv(bigDataPath / 'Test.csv')\n",
    "\n",
    "for file in tqdm(sorted(testPath.glob('**/*.png'))):\n",
    "    if file.parent.name != '.ipynb_checkpoints':\n",
    "        image = cv2.imread(str(file))\n",
    "        image = cv2.resize(image, (32, 32))\n",
    "        X_test.append(image)\n",
    "        \n",
    "        label = int(df.loc[df['Path'] == f'Test/{file.name}'].ClassId.item())\n",
    "        y_test.append(label)\n",
    "    \n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_test, y_test = shuffle(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0559c-1b68-4593-a896-4ce83bbbeb58",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7098d370-7a3f-4e84-bda0-a50172132981",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path.cwd()\n",
    "\n",
    "OPT = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "DATASET_INFO = {\n",
    "    'mnist': {\n",
    "        'class_names': [str(i) for i in range(10)],\n",
    "        'input_shape': (28, 28, 1)\n",
    "    },\n",
    "    'fashion_mnist': {\n",
    "        'class_names': ['t-shirt/top', 'trouser', 'pullover', 'dress', 'coat', \n",
    "                        'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot'],\n",
    "        'input_shape': (28, 28, 1)\n",
    "    },\n",
    "    'cifar10':{\n",
    "        'class_names': ['airplane', 'car', 'bird', 'cat', 'deer', \n",
    "                        'dog', 'frog', 'horse', 'ship', 'truck'],\n",
    "        'input_shape': (32, 32, 3)\n",
    "    },\n",
    "    'gtsrb':{\n",
    "        'class_names': [i for i in range(43)],\n",
    "        'input_shape': (32, 32, 3)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c7e10-43d3-4db0-8a66-e11217ddbd64",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6fa5d6b-bf82-4394-8b88-224846166978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data=None, dataset_name='mnist', n=5, \n",
    "              save=False, show=False,\n",
    "              name=None, path='/plot_data/',\n",
    "              fontsize=14, axis='off', cmap='gray_r'):\n",
    "\n",
    "    images, labels = data\n",
    "    figsize = (n*2, n*2)\n",
    "    plt.figure(figsize=(figsize))\n",
    "    for i in range(n**2):\n",
    "        plt.subplot(n, n, i+1)\n",
    "        plt.axis(axis)\n",
    "        plt.imshow(images[i].squeeze(), cmap=cmap)\n",
    "        if labels is not None:\n",
    "            plt.title(DATASET_INFO[dataset_name]['class_names'][labels[i].squeeze()], fontsize=fontsize)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        while name is None:\n",
    "            name = input(\"Enter name for figure: \")\n",
    "\n",
    "        file_path = cwd / Path('figures') / Path(path)\n",
    "        file_path.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(file_path.joinpath(str(name) + '.png'))\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3499b8a-f7c4-4bf9-833b-938a825e873a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train, test = mnist.load_data()\n",
    "train, test = (X_train, y_train), (X_test, y_test)\n",
    "plot_data(data=train, dataset_name='gtsrb', show=True, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c962d-44c6-4e32-ad2e-e53310440fe0",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28773d57-c826-4719-9a41-47c627cd1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(dataset_name='mnist', \n",
    "                         n_classes=10, opt=OPT):\n",
    "\n",
    "    in_shape = DATASET_INFO[dataset_name]['input_shape']\n",
    "    \n",
    "    # LABEL INPUT\n",
    "    in_label = Input(shape=(1,))\n",
    "    \n",
    "    # EMBEDDING FOR CATEGORICAL INPUT\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "\n",
    "    n_nodes = in_shape[0] * in_shape[1] * 1\n",
    "    li = Dense(n_nodes)(li)\n",
    "\n",
    "    # RESHAPING TO MATCH THE DIMENSIONS OF THE IMAGE\n",
    "    li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "    \n",
    "    # INPUT OF THE IMAGE\n",
    "    in_image = Input(shape=in_shape)\n",
    "    \n",
    "    # CONCATENATE LABEL TO THE CHANNELS\n",
    "    merge = Concatenate()([in_image, li])\n",
    "\n",
    "    # DOWNSAMPLE 1\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    if in_shape[2] == 3: assert fe.shape == (None, 16, 16, 128)\n",
    "    if in_shape[2] == 1: assert fe.shape == (None, 14, 14, 128)\n",
    "    \n",
    "    # DOWNSAMPLE 2\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)    \n",
    "    if in_shape[2] == 3: assert fe.shape == (None, 8, 8, 128)\n",
    "    if in_shape[2] == 1: assert fe.shape == (None, 7, 7, 128)\n",
    "\n",
    "    # DOWNSAMPLE 3\n",
    "    if in_shape[2] == 3:\n",
    "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)    \n",
    "        assert fe.shape == (None, 4, 4, 128)\n",
    "    \n",
    "    # FLATTEN\n",
    "    fe = Flatten()(fe)\n",
    "    # DROPOUT\n",
    "    fe = Dropout(0.4)(fe)\n",
    "    # OUTPUT\n",
    "    out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "    \n",
    "    model = Model([in_image, in_label], out_layer)                            \n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=opt, \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de1a1f25-9c61-44e2-a811-db15fde81529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(dataset_name='mnist',\n",
    "                     latent_dim=100, \n",
    "                     n_classes=10):\n",
    "\n",
    "    input_shape = DATASET_INFO[dataset_name]['input_shape']\n",
    "    in_label = Input(shape=(1,))\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "\n",
    "    node = 7 \n",
    "    if input_shape[2] == 3:\n",
    "        node = 4\n",
    "\n",
    "    n_nodes = node * node\n",
    "\n",
    "    li = Dense(n_nodes)(li)\n",
    "    li = Reshape((node, node, 1))(li)\n",
    "\n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "    \n",
    "    # FOUNDATION FOR THE IMAGE\n",
    "    n_nodes = 128 * node * node\n",
    "    gen = Dense(n_nodes)(in_lat)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = Reshape((node, node, 128))(gen)\n",
    "    # merge image gen and label input\n",
    "    merge = Concatenate()([gen, li])\n",
    "\n",
    "    # UPSAMPLE 1\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    if input_shape[2] == 1: assert gen.shape == (None, 14, 14, 128)\n",
    "    if input_shape[2] == 3: assert gen.shape == (None, 8, 8, 128)\n",
    "    \n",
    "    # UPSAMPLE 2\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    if input_shape[2] == 1: assert gen.shape == (None, 28, 28, 128)\n",
    "    if input_shape[2] == 3: assert gen.shape == (None, 16, 16, 128)\n",
    "\n",
    "    if input_shape[2] == 3:\n",
    "        gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        if input_shape[2] == 3: assert gen.shape == (None, 32, 32, 128)\n",
    "    \n",
    "    # OUTPUT\n",
    "    out_layer = Conv2D(input_shape[2], (7,7), activation='tanh', padding='same')(gen)\n",
    "    assert out_layer.shape == (None, input_shape[0], input_shape[1], input_shape[2])\n",
    "    \n",
    "    model = Model([in_lat, in_label], out_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbae1855-9f5b-4d4c-b0ef-edf9d9d3721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model ,opt=OPT):\n",
    "    d_model.trainable = False\n",
    "    \n",
    "    # GET NOISE AND LABELS FROM GENERATOR\n",
    "    gen_noise, gen_label = g_model.input\n",
    "    \n",
    "    # IMAGE OUTPUT FROM GENERATOR\n",
    "    gen_output = g_model.output\n",
    "    \n",
    "    # CONNECTING IMAGE OUTPUT AND LABEL INPUT FROM THE GENERATOR -> DISCRIMINATOR\n",
    "    gan_output = d_model([gen_output, gen_label])\n",
    "    \n",
    "    # GAN MODEL -> TAKING IN NOISE AND LABEL AND OUTPUTS A CLASSIFICATION\n",
    "    model = Model([gen_noise, gen_label], gan_output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23de85c-5966-4a8a-8c7e-bce130070259",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b753375a-08d1-42fe-8c51-b4bc42bf7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples(dataset_name='mnist', \n",
    "                      dataset=mnist):\n",
    "    input_shape = DATASET_INFO[dataset_name]['input_shape']\n",
    "\n",
    "    bigDataPath = Path('/Users/hamzz/Development/bigData/GTSRB')\n",
    "\n",
    "    trainPath = bigDataPath / Path('Train')\n",
    "    testPath = bigDataPath / Path('Test')\n",
    "    \n",
    "    X_train, y_train = [], []\n",
    "    X_test, y_test = [], []\n",
    "    \n",
    "    for file in tqdm(sorted(trainPath.glob('**/*.png'))):\n",
    "        if file.parent.name != '.ipynb_checkpoints':\n",
    "            image = cv2.imread(str(file))\n",
    "            image = cv2.resize(image, (32, 32))\n",
    "            X_train.append(image)\n",
    "            y_train.append(int(file.parent.name))\n",
    "    \n",
    "    df = pd.read_csv(bigDataPath / 'Test.csv')\n",
    "    \n",
    "    for file in tqdm(sorted(testPath.glob('**/*.png'))):\n",
    "        if file.parent.name != '.ipynb_checkpoints':\n",
    "            image = cv2.imread(str(file))\n",
    "            image = cv2.resize(image, (32, 32))\n",
    "            X_test.append(image)\n",
    "            \n",
    "            label = int(df.loc[df['Path'] == f'Test/{file.name}'].ClassId.item())\n",
    "            y_test.append(label)\n",
    "        \n",
    "    X_train = np.asarray(X_train)\n",
    "    X_test = np.asarray(X_test)\n",
    "    \n",
    "    y_train = np.asarray(y_train)\n",
    "    y_test = np.asarray(y_test)\n",
    "    \n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    X_test, y_test = shuffle(X_test, y_test)\n",
    "                          \n",
    "    # Append the augmented data to the original dataset\n",
    "    x_train = np.concatenate([X_train, X_test])\n",
    "    y_train = np.concatenate([y_train, y_test])\n",
    "    \n",
    "    input_shape = DATASET_INFO[dataset_name]['input_shape']\n",
    "    x_train = (x_train.reshape(x_train.shape[0], input_shape[0], input_shape[1], input_shape[2]).astype('float32') - 127.5) / 127.5\n",
    "                          \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5ac6bbc-9051-4d75-8ba0-cbe8fac1ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "    # choose random instances\n",
    "    ix = randint(0, images.shape[0], n_samples)\n",
    "    # select images and labels\n",
    "    X, labels = images[ix], labels[ix]\n",
    "    # generate class labels\n",
    "    y = ones((n_samples, 1))\n",
    "    return [X, labels], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3be2364-8627-49bb-848d-a8ef80bff1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples, n_classes=10):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    # generate labels\n",
    "    labels = randint(0, n_classes, n_samples)\n",
    "    return [z_input, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0296940-61dd-4ca7-a538-192be919f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    images = generator.predict([z_input, labels_input])\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return [images, labels_input], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a876105-a314-4b6f-bf18-16c88fd79cc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m g_model \u001b[38;5;241m=\u001b[39m define_generator()\n\u001b[0;32m----> 3\u001b[0m (X, y) \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# X, y = generate_latent_points(latent_dim=100, n_samples=1, n_classes=10)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# X = g_model.predict([X, y])\u001b[39;00m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "g_model = define_generator()\n",
    "\n",
    "(X, y) = dataset\n",
    "# X, y = generate_latent_points(latent_dim=100, n_samples=1, n_classes=10)\n",
    "# X = g_model.predict([X, y])\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(X[i], cmap='gray_r')\n",
    "    plt.title(y[i])\n",
    "    plt.axis('off')\n",
    "plt.savefig('training_data.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c9e1d2-5890-4bfb-97d8-bc3f9aed4647",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd769660-298f-4c79-8489-a80d6aef8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(g_model, epoch, path=None):\n",
    "    file_path = cwd / Path('models') / Path(path)\n",
    "    file_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    g_model.save(file_path / f'gen_model_e-{epoch+1:03d}.h5')\n",
    "    \n",
    "def summarise_performance(epoch, g_model, latent_dim, n_samples=100):\n",
    "    [X, labels], y = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    X = (X + 1) / 2.0\n",
    "    \n",
    "    plot_data(data=(X, labels), dataset_name='gtsrb', save=True, \n",
    "              axis='off', path=f'{GAN_NAME}/images', \n",
    "              name=f'gen_image_e-{epoch+1:03d}', show=True)\n",
    "    \n",
    "    save_model(g_model, epoch, path=f'{GAN_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cb1351d-d717-440a-8457-058a0d714fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128):\n",
    "    bat_per_epoch = dataset[0].shape[0] // n_batch\n",
    "    half_batch = n_batch // 2\n",
    "    hash = {\n",
    "        'd_loss1': [],\n",
    "        'd_loss2': [],\n",
    "        'g_loss': []\n",
    "    }\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(bat_per_epoch):\n",
    "            [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
    "            d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
    "            hash['d_loss1'].append(d_loss1)\n",
    "            \n",
    "            [X_fake, labels_fake], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            d_loss2, _ = d_model.train_on_batch([X_fake, labels_fake], y_fake)\n",
    "            hash['d_loss2'].append(d_loss2)\n",
    "            \n",
    "            [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            \n",
    "            g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
    "            hash['g_loss'].append(g_loss)\n",
    "\n",
    "            print(f\">[{i+1:03d}/{n_epochs}] | Batch: [{j+1:03d}/{bat_per_epoch:03d}] | D-Real: {d_loss1:.3f} | D-Fake: {d_loss2:.3f} | G: {g_loss:.3f}\")\n",
    "\n",
    "            if j % 30 == 0:\n",
    "                clear_output(wait=True)\n",
    "        summarise_performance(i, g_model, latent_dim)\n",
    "    return hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0851002c-2bac-4c96-9680-15ab601710de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5afb0c28abb46a088f4b24c55737ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc96b0b2d6034f2aa6cfee6896d3ed59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latent_dim = 100\n",
    "d_model = define_discriminator(dataset_name='gtsrb')\n",
    "g_model = define_generator(latent_dim=latent_dim, dataset_name='gtsrb')\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "\n",
    "GAN_NAME = f'cgan/gtsrb'\n",
    "dataset = load_real_samples(dataset=None, dataset_name='gtsrb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9b52e0f-1cfc-44f0-a368-798f3d126bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 18ms/step\n",
      ">[007/200] | Batch: [122/202] | D-Real: 0.374 | D-Fake: 0.294 | G: 1.639\n",
      "4/4 [==============================] - 0s 18ms/step\n",
      ">[007/200] | Batch: [123/202] | D-Real: 0.422 | D-Fake: 0.288 | G: 1.584\n",
      "4/4 [==============================] - 0s 17ms/step\n",
      ">[007/200] | Batch: [124/202] | D-Real: 0.413 | D-Fake: 0.272 | G: 1.657\n",
      "4/4 [==============================] - 0s 17ms/step\n",
      ">[007/200] | Batch: [125/202] | D-Real: 0.344 | D-Fake: 0.274 | G: 1.675\n",
      "4/4 [==============================] - 0s 18ms/step\n",
      ">[007/200] | Batch: [126/202] | D-Real: 0.431 | D-Fake: 0.250 | G: 1.688\n",
      "4/4 [==============================] - 0s 18ms/step\n",
      ">[007/200] | Batch: [127/202] | D-Real: 0.459 | D-Fake: 0.269 | G: 1.689\n",
      "4/4 [==============================] - 0s 18ms/step\n",
      ">[007/200] | Batch: [128/202] | D-Real: 0.500 | D-Fake: 0.285 | G: 1.617\n",
      "4/4 [==============================] - 0s 18ms/step\n",
      ">[007/200] | Batch: [129/202] | D-Real: 0.424 | D-Fake: 0.281 | G: 1.577\n",
      "4/4 [==============================] - 0s 17ms/step\n",
      ">[007/200] | Batch: [130/202] | D-Real: 0.461 | D-Fake: 0.286 | G: 1.578\n",
      "4/4 [==============================] - 0s 17ms/step\n",
      ">[007/200] | Batch: [131/202] | D-Real: 0.419 | D-Fake: 0.300 | G: 1.642\n",
      "4/4 [==============================] - 0s 18ms/step\n",
      ">[007/200] | Batch: [132/202] | D-Real: 0.387 | D-Fake: 0.312 | G: 1.645\n",
      "4/4 [==============================] - 0s 17ms/step\n",
      ">[007/200] | Batch: [133/202] | D-Real: 0.452 | D-Fake: 0.301 | G: 1.565\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mhash\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgan_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(g_model, d_model, gan_model, dataset, latent_dim, n_epochs, n_batch)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(bat_per_epoch):\n\u001b[1;32m     11\u001b[0m     [X_real, labels_real], y_real \u001b[38;5;241m=\u001b[39m generate_real_samples(dataset, half_batch)\n\u001b[0;32m---> 12\u001b[0m     d_loss1, _ \u001b[38;5;241m=\u001b[39m \u001b[43md_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_real\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_real\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mhash\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_loss1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(d_loss1)\n\u001b[1;32m     15\u001b[0m     [X_fake, labels_fake], y_fake \u001b[38;5;241m=\u001b[39m generate_fake_samples(g_model, latent_dim, half_batch)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/keras/engine/training.py:2502\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2500\u001b[0m _disallow_inside_tf_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_on_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_metrics:\n\u001b[0;32m-> 2502\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   2504\u001b[0m     \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   2505\u001b[0m ):\n\u001b[1;32m   2506\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[1;32m   2507\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[1;32m   2508\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/keras/engine/training.py:2448\u001b[0m, in \u001b[0;36mModel.reset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2429\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the state of all the metrics in the model.\u001b[39;00m\n\u001b[1;32m   2430\u001b[0m \n\u001b[1;32m   2431\u001b[0m \u001b[38;5;124;03mExamples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2445\u001b[0m \n\u001b[1;32m   2446\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics:\n\u001b[0;32m-> 2448\u001b[0m     \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/keras/metrics/base_metric.py:259\u001b[0m, in \u001b[0;36mMetric.reset_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_states()\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_set_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/keras/backend.py:4312\u001b[0m, in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, value \u001b[38;5;129;01min\u001b[39;00m tuples:\n\u001b[1;32m   4311\u001b[0m         value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(value, dtype\u001b[38;5;241m=\u001b[39mdtype_numpy(x))\n\u001b[0;32m-> 4312\u001b[0m         \u001b[43m_assign_value_to_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4314\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_graph()\u001b[38;5;241m.\u001b[39mas_default():\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/keras/backend.py:4360\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[0;34m(variable, value)\u001b[0m\n\u001b[1;32m   4357\u001b[0m     variable\u001b[38;5;241m.\u001b[39massign(d_value)\n\u001b[1;32m   4358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4359\u001b[0m     \u001b[38;5;66;03m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[0;32m-> 4360\u001b[0m     \u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:974\u001b[0m, in \u001b[0;36mBaseResourceVariable.assign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# Note: not depending on the cached value here since this can be used to\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# initialize the variable.\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _handle_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle):\n\u001b[0;32m--> 974\u001b[0m   value_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape\u001b[38;5;241m.\u001b[39mis_compatible_with(value_tensor\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1642\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1633\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1634\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1635\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1638\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1639\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1642\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1645\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     47\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:268\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:280\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    279\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 280\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[1;32m    283\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:305\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hash = train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=200, n_batch=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451b001-0582-437d-982d-5e6af777c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame.from_dict(hash).to_csv('gtsrb.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ade2fd-766a-490b-9579-21b9b9edff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5020db-68c7-4d70-845a-a062bb5545a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d3d49f-8f52-422d-9aca-8bb1991fbd57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
