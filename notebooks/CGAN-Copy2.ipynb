{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd3bda44-b63f-45db-a401-f0e56210c85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import mnist, fashion_mnist, cifar10\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Embedding, Dense, Reshape, Concatenate\n",
    "from keras.layers import Conv2D, LeakyReLU, Dropout, Flatten, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import ones, zeros, expand_dims, vstack, asarray\n",
    "from numpy import linspace, arccos, clip, sin, cos, dot\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import rand, randn, randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0559c-1b68-4593-a896-4ce83bbbeb58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7098d370-7a3f-4e84-bda0-a50172132981",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path.cwd()\n",
    "\n",
    "OPT = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "DATASET_INFO = {\n",
    "    'mnist': {\n",
    "        'class_names': [str(i) for i in range(10)],\n",
    "        'input_shape': (28, 28, 1)\n",
    "    },\n",
    "    'fashion_mnist': {\n",
    "        'class_names': ['t-shirt/top', 'trouser', 'pullover', 'dress', 'coat', \n",
    "                        'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot'],\n",
    "        'input_shape': (28, 28, 1)\n",
    "    },\n",
    "    'cifar10':{\n",
    "        'class_names': ['airplane', 'car', 'bird', 'cat', 'deer', \n",
    "                        'dog', 'frog', 'horse', 'ship', 'truck'],\n",
    "        'input_shape': (32, 32, 3)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6fa5d6b-bf82-4394-8b88-224846166978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data=None, dataset_name='mnist', n=5, \n",
    "              save=False, show=False,\n",
    "              name=None, path='/plot_data/',\n",
    "              fontsize=14, axis='off', cmap='gray_r'):\n",
    "\n",
    "    images, labels = data\n",
    "    figsize = (n*2, n*2)\n",
    "    plt.figure(figsize=(figsize))\n",
    "    for i in range(n**2):\n",
    "        plt.subplot(n, n, i+1)\n",
    "        plt.axis(axis)\n",
    "        plt.imshow(images[i].squeeze(), cmap=cmap)\n",
    "        if labels is not None:\n",
    "            plt.title(DATASET_INFO[dataset_name]['class_names'][labels[i].squeeze()], fontsize=fontsize)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        while name is None:\n",
    "            name = input(\"Enter name for figure: \")\n",
    "\n",
    "        file_path = cwd / Path('figures') / Path(path)\n",
    "        file_path.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(file_path.joinpath(str(name) + '.png'))\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3499b8a-f7c4-4bf9-833b-938a825e873a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, test = mnist.load_data()\n",
    "plot_data(data=train, show=True, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c962d-44c6-4e32-ad2e-e53310440fe0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28773d57-c826-4719-9a41-47c627cd1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(dataset_name='mnist', \n",
    "                         n_classes=10, opt=OPT):\n",
    "\n",
    "    in_shape = DATASET_INFO[dataset_name]['input_shape']\n",
    "    \n",
    "    # LABEL INPUT\n",
    "    in_label = Input(shape=(1,))\n",
    "    \n",
    "    # EMBEDDING FOR CATEGORICAL INPUT\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "\n",
    "    n_nodes = in_shape[0] * in_shape[1] * 1\n",
    "    li = Dense(n_nodes)(li)\n",
    "\n",
    "    # RESHAPING TO MATCH THE DIMENSIONS OF THE IMAGE\n",
    "    li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "    \n",
    "    # INPUT OF THE IMAGE\n",
    "    in_image = Input(shape=in_shape)\n",
    "    \n",
    "    # CONCATENATE LABEL TO THE CHANNELS\n",
    "    merge = Concatenate()([in_image, li])\n",
    "\n",
    "    # DOWNSAMPLE 1\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    if in_shape[2] == 3: assert fe.shape == (None, 16, 16, 128)\n",
    "    if in_shape[2] == 1: assert fe.shape == (None, 14, 14, 128)\n",
    "    \n",
    "    # DOWNSAMPLE 2\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)    \n",
    "    if in_shape[2] == 3: assert fe.shape == (None, 8, 8, 128)\n",
    "    if in_shape[2] == 1: assert fe.shape == (None, 7, 7, 128)\n",
    "\n",
    "    # DOWNSAMPLE 3\n",
    "    if in_shape[2] == 3:\n",
    "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)    \n",
    "        assert fe.shape == (None, 4, 4, 128)\n",
    "    \n",
    "    # FLATTEN\n",
    "    fe = Flatten()(fe)\n",
    "    # DROPOUT\n",
    "    fe = Dropout(0.4)(fe)\n",
    "    # OUTPUT\n",
    "    out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "    \n",
    "    model = Model([in_image, in_label], out_layer)                            \n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=opt, \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de1a1f25-9c61-44e2-a811-db15fde81529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(dataset_name='mnist',\n",
    "                     latent_dim=100, \n",
    "                     n_classes=10):\n",
    "\n",
    "    input_shape = DATASET_INFO[dataset_name]['input_shape']\n",
    "    in_label = Input(shape=(1,))\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "\n",
    "    node = 7 \n",
    "    if input_shape[2] == 3:\n",
    "        node = 4\n",
    "\n",
    "    n_nodes = node * node\n",
    "\n",
    "    li = Dense(n_nodes)(li)\n",
    "    li = Reshape((node, node, 1))(li)\n",
    "\n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "    \n",
    "    # FOUNDATION FOR THE IMAGE\n",
    "    n_nodes = 128 * node * node\n",
    "    gen = Dense(n_nodes)(in_lat)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = Reshape((node, node, 128))(gen)\n",
    "    # merge image gen and label input\n",
    "    merge = Concatenate()([gen, li])\n",
    "\n",
    "    # UPSAMPLE 1\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    if input_shape[2] == 1: assert gen.shape == (None, 14, 14, 128)\n",
    "    if input_shape[2] == 3: assert gen.shape == (None, 8, 8, 128)\n",
    "    \n",
    "    # UPSAMPLE 2\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    if input_shape[2] == 1: assert gen.shape == (None, 28, 28, 128)\n",
    "    if input_shape[2] == 3: assert gen.shape == (None, 16, 16, 128)\n",
    "\n",
    "    if input_shape[2] == 3:\n",
    "        gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        if input_shape[2] == 3: assert gen.shape == (None, 32, 32, 128)\n",
    "    \n",
    "    # OUTPUT\n",
    "    out_layer = Conv2D(input_shape[2], (7,7), activation='tanh', padding='same')(gen)\n",
    "    assert out_layer.shape == (None, input_shape[0], input_shape[1], input_shape[2])\n",
    "    \n",
    "    model = Model([in_lat, in_label], out_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbae1855-9f5b-4d4c-b0ef-edf9d9d3721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model ,opt=OPT):\n",
    "    d_model.trainable = False\n",
    "    \n",
    "    # GET NOISE AND LABELS FROM GENERATOR\n",
    "    gen_noise, gen_label = g_model.input\n",
    "    \n",
    "    # IMAGE OUTPUT FROM GENERATOR\n",
    "    gen_output = g_model.output\n",
    "    \n",
    "    # CONNECTING IMAGE OUTPUT AND LABEL INPUT FROM THE GENERATOR -> DISCRIMINATOR\n",
    "    gan_output = d_model([gen_output, gen_label])\n",
    "    \n",
    "    # GAN MODEL -> TAKING IN NOISE AND LABEL AND OUTPUTS A CLASSIFICATION\n",
    "    model = Model([gen_noise, gen_label], gan_output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23de85c-5966-4a8a-8c7e-bce130070259",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b753375a-08d1-42fe-8c51-b4bc42bf7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples(dataset_name='mnist', \n",
    "                      dataset=mnist):\n",
    "    input_shape = DATASET_INFO[dataset_name]['input_shape']\n",
    "    \n",
    "    # Load the MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = dataset.load_data()\n",
    "    \n",
    "    # Reshape and normalize the input data\n",
    "    x_train = (x_train.reshape(x_train.shape[0], input_shape[0], input_shape[1], input_shape[2]).astype('float32') - 127.5) / 127.5\n",
    "    x_test = (x_test.reshape(x_test.shape[0], input_shape[0], input_shape[1], input_shape[2]).astype('float32') - 127.5) / 127.5\n",
    "                          \n",
    "    # Define the augmentation parameters\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    # Generate augmented images and labels\n",
    "    augmented_data = datagen.flow(x_train, y_train, batch_size=len(x_train), seed=42)\n",
    "    augmented_data_test = datagen.flow(x_test, y_test, batch_size=len(x_test), seed=22)\n",
    "                          \n",
    "    augmented_images, augmented_labels = augmented_data.next()\n",
    "    augmented_images_test, augmented_labels_test = augmented_data_test.next()\n",
    "    \n",
    "    # Append the augmented data to the original dataset\n",
    "    x_train_augmented = np.concatenate([x_train, augmented_images, augmented_images_test])\n",
    "    y_train_augmented = np.concatenate([y_train, augmented_labels, augmented_labels_test])\n",
    "    return x_train_augmented, y_train_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5ac6bbc-9051-4d75-8ba0-cbe8fac1ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "    # choose random instances\n",
    "    ix = randint(0, images.shape[0], n_samples)\n",
    "    # select images and labels\n",
    "    X, labels = images[ix], labels[ix]\n",
    "    # generate class labels\n",
    "    y = ones((n_samples, 1))\n",
    "    return [X, labels], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3be2364-8627-49bb-848d-a8ef80bff1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples, n_classes=10):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    # generate labels\n",
    "    labels = randint(0, n_classes, n_samples)\n",
    "    return [z_input, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0296940-61dd-4ca7-a538-192be919f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    images = generator.predict([z_input, labels_input])\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return [images, labels_input], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd769660-298f-4c79-8489-a80d6aef8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(g_model, epoch, path=None):\n",
    "    file_path = cwd / Path('models') / Path(path)\n",
    "    file_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    g_model.save(file_path / f'gen_model_e-{epoch+1:03d}.h5')\n",
    "    \n",
    "def summarise_performance(epoch, g_model, latent_dim, n_samples=100):\n",
    "    [X, labels], y = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    X = (X + 1) / 2.0\n",
    "\n",
    "    plot_data(data=(X, labels), dataset_name='fashion_mnist', save=True, \n",
    "              axis='off', path=f'{GAN_NAME}/images', \n",
    "              name=f'gen_image_e-{epoch+1:03d}', show=True)\n",
    "    \n",
    "    save_model(g_model, epoch, path=f'{GAN_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cb1351d-d717-440a-8457-058a0d714fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128):\n",
    "    bat_per_epoch = dataset[0].shape[0] // n_batch\n",
    "    half_batch = n_batch // 2\n",
    "    hash = {\n",
    "        'd_loss1': [],\n",
    "        'd_loss2': [],\n",
    "        'g_loss': []\n",
    "    }\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(bat_per_epoch):\n",
    "            [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
    "            d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
    "            hash['d_loss1'].append(d_loss1)\n",
    "            \n",
    "            [X_fake, labels_fake], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            d_loss2, _ = d_model.train_on_batch([X_fake, labels_fake], y_fake)\n",
    "            hash['d_loss2'].append(d_loss2)\n",
    "            \n",
    "            [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            \n",
    "            g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
    "            hash['g_loss'].append(g_loss)\n",
    "\n",
    "            print(f\">[{i+1:03d}/{n_epochs}] | Batch: [{j+1:03d}/{bat_per_epoch:03d}] | D-Real: {d_loss1:.3f} | D-Fake: {d_loss2:.3f} | G: {g_loss:.3f}\")\n",
    "\n",
    "            if j % 30 == 0:\n",
    "                clear_output(wait=True)\n",
    "        summarise_performance(i, g_model, latent_dim)\n",
    "    return hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0851002c-2bac-4c96-9680-15ab601710de",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "d_model = define_discriminator(dataset_name='fashion_mnist')\n",
    "g_model = define_generator(latent_dim=latent_dim, dataset_name='fashion_mnist')\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "\n",
    "GAN_NAME = f'cgan/fashion_mnist/E_1000'\n",
    "dataset = load_real_samples(dataset=fashion_mnist, dataset_name='fashion_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b52e0f-1cfc-44f0-a368-798f3d126bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [362/507] | D-Real: 0.696 | D-Fake: 0.693 | G: 0.700\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [363/507] | D-Real: 0.700 | D-Fake: 0.688 | G: 0.731\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [364/507] | D-Real: 0.695 | D-Fake: 0.672 | G: 0.752\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [365/507] | D-Real: 0.702 | D-Fake: 0.679 | G: 0.729\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [366/507] | D-Real: 0.702 | D-Fake: 0.694 | G: 0.716\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [367/507] | D-Real: 0.694 | D-Fake: 0.690 | G: 0.730\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [368/507] | D-Real: 0.691 | D-Fake: 0.677 | G: 0.731\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [369/507] | D-Real: 0.694 | D-Fake: 0.679 | G: 0.715\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [370/507] | D-Real: 0.686 | D-Fake: 0.689 | G: 0.712\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [371/507] | D-Real: 0.696 | D-Fake: 0.695 | G: 0.717\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [372/507] | D-Real: 0.686 | D-Fake: 0.683 | G: 0.713\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [373/507] | D-Real: 0.688 | D-Fake: 0.688 | G: 0.718\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [374/507] | D-Real: 0.697 | D-Fake: 0.686 | G: 0.702\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [375/507] | D-Real: 0.687 | D-Fake: 0.691 | G: 0.710\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [376/507] | D-Real: 0.706 | D-Fake: 0.689 | G: 0.723\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [377/507] | D-Real: 0.701 | D-Fake: 0.680 | G: 0.723\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [378/507] | D-Real: 0.685 | D-Fake: 0.669 | G: 0.713\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [379/507] | D-Real: 0.690 | D-Fake: 0.697 | G: 0.715\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [380/507] | D-Real: 0.681 | D-Fake: 0.695 | G: 0.721\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [381/507] | D-Real: 0.689 | D-Fake: 0.698 | G: 0.728\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [382/507] | D-Real: 0.699 | D-Fake: 0.699 | G: 0.724\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      ">[537/1000] | Batch: [383/507] | D-Real: 0.699 | D-Fake: 0.688 | G: 0.716\n",
      "4/4 [==============================] - 0s 28ms/step\n",
      ">[537/1000] | Batch: [384/507] | D-Real: 0.693 | D-Fake: 0.684 | G: 0.711\n",
      "4/4 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "hash = train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=1000, n_batch=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4df57a-cad4-4f76-a5f3-af8eb12965cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame.from_dict(hash).to_csv('fm-2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16185d3-b873-4174-873a-9b70fd2d462b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
