{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435654dc-bf30-4801-ad53-3b509d1d8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.datasets import mnist\n",
    "# from tensorflow.keras.datasets.mnist import load_data\n",
    "\n",
    "# from keras.datasets import mnist, cifar10, fashion_mnist\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose\n",
    "from keras.layers import LeakyReLU, Dropout, Embedding, Concatenate, BatchNormalization\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1edd80dd-e5a9-4347-8ac5-f6cec2932ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = {\n",
    "    'mnist': [f'{i}' for i in range(10)],\n",
    "    'fashion_mnist': ['t-shirt/top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot'],\n",
    "    'cifar10': ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'],\n",
    "}\n",
    "\n",
    "cwd = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1278c3d9-828a-4618-838c-da2d85066145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(in_shape=(28,28,1), n_classes=10):\n",
    "    in_label = Input(shape=(1,))\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "    n_nodes = in_shape[0] * in_shape[1]\n",
    "    li = Dense(n_nodes)(li)\n",
    "    li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "    \n",
    "    in_image = Input(shape=in_shape)\n",
    "    \n",
    "    merge = Concatenate()([in_image, li])\n",
    "    \n",
    "    # downsample\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    \n",
    "    # downsample\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    # flatten feature maps\n",
    "    fe = Flatten()(fe)\n",
    "    # dropout\n",
    "    fe = Dropout(0.4)(fe)\n",
    "    # output\n",
    "    out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "    # define model\n",
    "    model = Model([in_image, in_label], out_layer)\n",
    "    \n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def define_generator(latent_dim, n_classes=10):\n",
    "    \n",
    "    in_label = Input(shape=(1,))\n",
    "    \n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "    \n",
    "    n_nodes = 7 * 7\n",
    "    li = Dense(n_nodes)(li)\n",
    "    \n",
    "    li = Reshape((7, 7, 1))(li)\n",
    "    \n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "    \n",
    "    n_nodes = 128 * 7 * 7\n",
    "    gen = Dense(n_nodes)(in_lat)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = Reshape((7, 7, 128))(gen)\n",
    "    \n",
    "    merge = Concatenate()([gen, li])\n",
    "    \n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    \n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    \n",
    "    out_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
    "    \n",
    "    model = Model([in_lat, in_label], out_layer)\n",
    "    return model\n",
    "\n",
    "def define_gan(g_model, d_model):\n",
    "    \n",
    "    d_model.trainable = False\n",
    "    \n",
    "    gen_noise, gen_label = g_model.input\n",
    "    \n",
    "    gen_output = g_model.output\n",
    "    \n",
    "    gan_output = d_model([gen_output, gen_label])\n",
    "    \n",
    "    model = Model([gen_noise, gen_label], gan_output)\n",
    "    \n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30b1fd6c-c7b5-4045-b626-4d2a45d8dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fashion mnist images\n",
    "def load_real_samples(dataset=None):\n",
    "    (trainX, trainy), (_, _) = dataset.load_data()\n",
    "    \n",
    "    X = expand_dims(trainX, axis=-1)\n",
    "    X = X.astype('float32')\n",
    "    X = (X - 127.5) / 127.5\n",
    "    \n",
    "    return [X, trainy]\n",
    "\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    images, labels = dataset\n",
    "    ix = randint(0, images.shape[0], n_samples)\n",
    "    X, labels = images[ix], labels[ix]\n",
    "    y = ones((n_samples, 1))\n",
    "    return [X, labels], y\n",
    "\n",
    "def generate_latent_points(latent_dim, n_samples, n_classes=10):\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    labels = randint(0, n_classes, n_samples)\n",
    "    return [z_input, labels]\n",
    "\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    images = generator.predict([z_input, labels_input])\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return [images, labels_input], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04226b79-8127-4502-b119-517f4f0a65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data=None,\n",
    "              n=5,\n",
    "              figsize=(10, 8), \n",
    "              save=False, \n",
    "              name=None, path='/plot_data/',\n",
    "              axis='on', show=False):\n",
    "\n",
    "    images, labels = data\n",
    "                  \n",
    "    plt.figure(figsize=(figsize))\n",
    "    for i in range(n**2):\n",
    "        plt.subplot(n, n, i+1)\n",
    "        plt.axis(axis)\n",
    "        plt.imshow(images[i].squeeze(), cmap='gray_r')\n",
    "        if labels is not None:\n",
    "            plt.title(CLASS_NAMES[DATASET_NAME][labels[i].squeeze()], fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        while name is None:\n",
    "            name = input(\"Enter name for figure: \")\n",
    "\n",
    "        file_path = cwd / Path('figures') / Path(path)\n",
    "        file_path.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(file_path.joinpath(str(name) + '.png'))\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def save_model(g_model, epoch, path=None):\n",
    "    file_path = cwd / Path('models') / Path(path)\n",
    "    file_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    g_model.save(file_path / f'gen_model_e-{epoch+1:03d}.h5')\n",
    "\n",
    "def summarise_performance(epoch, g_model, latent_dim, n_samples=100):\n",
    "    [X, labels], y = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    X = (X + 1) / 2.0\n",
    "\n",
    "    plot_data(data=(X, labels), save=True, axis='off', path=f'{GAN_NAME}/images', \n",
    "             name=f'gen_image_e-{epoch+1:03d}')\n",
    "    \n",
    "    save_model(g_model, epoch, path=f'{GAN_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c04dfba-1e16-4ce8-b9e5-7d029e4c632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128):\n",
    "    hash = {\n",
    "        'd_loss1': [],\n",
    "        'd_loss2': [],\n",
    "        'g_loss': [],\n",
    "    }\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)    \n",
    "    for i in range(n_epochs):        \n",
    "        for j in range(bat_per_epo):\n",
    "            \n",
    "            [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)            \n",
    "            d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
    "            hash['d_loss1'].append(d_loss1)\n",
    "            \n",
    "            [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)            \n",
    "            d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
    "            hash['d_loss2'].append(d_loss2)\n",
    "            \n",
    "            [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)            \n",
    "            y_gan = ones((n_batch, 1))\n",
    "            g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
    "            hash['g_loss'].append(g_loss)\n",
    "            \n",
    "            print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
    "            if j % 35 == 0:\n",
    "                clear_output(wait=True)\n",
    "        summarise_performance(i, g_model, latent_dim)\n",
    "    # save the generator model\n",
    "    g_model.save('cgan_generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3293257d-bfcc-41a9-a62e-5d4dbf117326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 37/468, d1=0.000, d2=0.000 g=0.019\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 38/468, d1=0.000, d2=0.000 g=0.019\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 39/468, d1=0.000, d2=0.000 g=0.018\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 40/468, d1=0.000, d2=0.000 g=0.019\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 41/468, d1=0.000, d2=0.000 g=0.019\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 42/468, d1=0.000, d2=0.000 g=0.019\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 43/468, d1=0.000, d2=0.000 g=0.020\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 44/468, d1=0.000, d2=0.000 g=0.019\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 45/468, d1=0.000, d2=0.000 g=0.021\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 46/468, d1=0.000, d2=0.000 g=0.021\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 47/468, d1=0.000, d2=0.000 g=0.020\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 48/468, d1=0.000, d2=0.000 g=0.019\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 49/468, d1=0.000, d2=0.000 g=0.019\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 50/468, d1=0.000, d2=0.000 g=0.020\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      ">4, 51/468, d1=0.000, d2=0.000 g=0.019\n",
      "2/2 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x12a6aa940>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hamzz/anaconda3/envs/tensorflow_env/lib/python3.8/weakref.py\", line 345, in remove\n",
      "    def remove(k, selfref=ref(self)):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m DATASET_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfashion_mnist\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m GAN_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCGAN/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mhash\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgan_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(g_model, d_model, gan_model, dataset, latent_dim, n_epochs, n_batch)\u001b[0m\n\u001b[1;32m     13\u001b[0m d_loss1, _ \u001b[38;5;241m=\u001b[39m d_model\u001b[38;5;241m.\u001b[39mtrain_on_batch([X_real, labels_real], y_real)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mhash\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_loss1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(d_loss1)\n\u001b[0;32m---> 16\u001b[0m [X_fake, labels], y_fake \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_fake_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalf_batch\u001b[49m\u001b[43m)\u001b[49m            \n\u001b[1;32m     17\u001b[0m d_loss2, _ \u001b[38;5;241m=\u001b[39m d_model\u001b[38;5;241m.\u001b[39mtrain_on_batch([X_fake, labels], y_fake)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mhash\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_loss2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(d_loss2)\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mgenerate_fake_samples\u001b[0;34m(generator, latent_dim, n_samples)\u001b[0m\n\u001b[1;32m     26\u001b[0m z_input, labels_input \u001b[38;5;241m=\u001b[39m generate_latent_points(latent_dim, n_samples)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# predict outputs\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mz_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# create class labels\u001b[39;00m\n\u001b[1;32m     30\u001b[0m y \u001b[38;5;241m=\u001b[39m zeros((n_samples, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/keras/engine/training.py:2416\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2408\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected result of `predict_function` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2409\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Empty batch_outputs). Please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2413\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue/bug to `tf.keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2414\u001b[0m         )\n\u001b[1;32m   2415\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_predict_end()\n\u001b[0;32m-> 2416\u001b[0m all_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure_up_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpotentially_ragged_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2420\u001b[0m \u001b[38;5;66;03m# If originally PSS strategy was used, then replace it back since\u001b[39;00m\n\u001b[1;32m   2421\u001b[0m \u001b[38;5;66;03m# predict is running under `OneDeviceStrategy` after the swap and once\u001b[39;00m\n\u001b[1;32m   2422\u001b[0m \u001b[38;5;66;03m# its done we need to replace it back to PSS again.\u001b[39;00m\n\u001b[1;32m   2423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_pss_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1435\u001b[0m, in \u001b[0;36mmap_structure_up_to\u001b[0;34m(shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.nest.map_structure_up_to\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure_up_to\u001b[39m(shallow_tree, func, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a function or op to a number of partially flattened inputs.\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \n\u001b[1;32m   1365\u001b[0m \u001b[38;5;124;03m  The `inputs` are flattened up to `shallow_tree` before being mapped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;124;03m    `shallow_tree`.\u001b[39;00m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1435\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_structure_with_tuple_paths_up_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshallow_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Discards the path arg.\u001b[39;49;00m\n\u001b[1;32m   1438\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1535\u001b[0m, in \u001b[0;36mmap_structure_with_tuple_paths_up_to\u001b[0;34m(shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m flat_value_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1527\u001b[0m     flatten_up_to(  \u001b[38;5;66;03m# pylint: disable=g-complex-comprehension\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m         shallow_tree,\n\u001b[1;32m   1529\u001b[0m         input_tree,\n\u001b[1;32m   1530\u001b[0m         check_types,\n\u001b[1;32m   1531\u001b[0m         expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites) \u001b[38;5;28;01mfor\u001b[39;00m input_tree \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[1;32m   1532\u001b[0m flat_path_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1533\u001b[0m     path\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m _yield_flat_up_to(shallow_tree, inputs[\u001b[38;5;241m0\u001b[39m], is_nested_fn))\n\u001b[0;32m-> 1535\u001b[0m results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1536\u001b[0m     func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_path_gen, \u001b[38;5;241m*\u001b[39mflat_value_gen)\n\u001b[1;32m   1537\u001b[0m ]\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(structure\u001b[38;5;241m=\u001b[39mshallow_tree, flat_sequence\u001b[38;5;241m=\u001b[39mresults,\n\u001b[1;32m   1539\u001b[0m                         expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1536\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1526\u001b[0m flat_value_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1527\u001b[0m     flatten_up_to(  \u001b[38;5;66;03m# pylint: disable=g-complex-comprehension\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m         shallow_tree,\n\u001b[1;32m   1529\u001b[0m         input_tree,\n\u001b[1;32m   1530\u001b[0m         check_types,\n\u001b[1;32m   1531\u001b[0m         expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites) \u001b[38;5;28;01mfor\u001b[39;00m input_tree \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[1;32m   1532\u001b[0m flat_path_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1533\u001b[0m     path\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m _yield_flat_up_to(shallow_tree, inputs[\u001b[38;5;241m0\u001b[39m], is_nested_fn))\n\u001b[1;32m   1535\u001b[0m results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m-> 1536\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_path_gen, \u001b[38;5;241m*\u001b[39mflat_value_gen)\n\u001b[1;32m   1537\u001b[0m ]\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(structure\u001b[38;5;241m=\u001b[39mshallow_tree, flat_sequence\u001b[38;5;241m=\u001b[39mresults,\n\u001b[1;32m   1539\u001b[0m                         expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1437\u001b[0m, in \u001b[0;36mmap_structure_up_to.<locals>.<lambda>\u001b[0;34m(_, *values)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.nest.map_structure_up_to\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure_up_to\u001b[39m(shallow_tree, func, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a function or op to a number of partially flattened inputs.\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \n\u001b[1;32m   1365\u001b[0m \u001b[38;5;124;03m  The `inputs` are flattened up to `shallow_tree` before being mapped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;124;03m    `shallow_tree`.\u001b[39;00m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m map_structure_with_tuple_paths_up_to(\n\u001b[1;32m   1436\u001b[0m       shallow_tree,\n\u001b[0;32m-> 1437\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m _, \u001b[38;5;241m*\u001b[39mvalues: \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# Discards the path arg.\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m       \u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m   1439\u001b[0m       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/keras/engine/training.py:3917\u001b[0m, in \u001b[0;36mpotentially_ragged_concat\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   3913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconcat(tensors, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   3915\u001b[0m non_batch_shapes \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mstack([tf\u001b[38;5;241m.\u001b[39mshape(tensor)[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors])\n\u001b[1;32m   3916\u001b[0m constant_dims \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mreduce_all(\n\u001b[0;32m-> 3917\u001b[0m     \u001b[43mnon_batch_shapes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnon_batch_shapes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3918\u001b[0m )\n\u001b[1;32m   3919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mreduce_all(constant_dims)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem():\n\u001b[1;32m   3920\u001b[0m     \u001b[38;5;66;03m# All non-batch dims are constant\u001b[39;00m\n\u001b[1;32m   3921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_scalar(tensors[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:2048\u001b[0m, in \u001b[0;36mtensor_equals\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (ops\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39m_USE_EQUALITY \u001b[38;5;129;01mand\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mexecuting_eagerly_outside_functions() \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   2046\u001b[0m     (g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m g\u001b[38;5;241m.\u001b[39mbuilding_function)):\n\u001b[1;32m   2047\u001b[0m   \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m maybe_promote_tensors(\u001b[38;5;28mself\u001b[39m, other)\n\u001b[0;32m-> 2048\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincompatible_shape_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m   \u001b[38;5;66;03m# In legacy graph mode, tensor equality is object equality\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m other\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py:3300\u001b[0m, in \u001b[0;36mequal\u001b[0;34m(x, y, incompatible_shape_error, name)\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3299\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3300\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3301\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEqual\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mincompatible_shape_error\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3302\u001b[0m \u001b[43m      \u001b[49m\u001b[43mincompatible_shape_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3304\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "latent_dim = 100\n",
    "\n",
    "d_model = define_discriminator()\n",
    "g_model = define_generator(latent_dim)\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "\n",
    "dataset = load_real_samples(dataset=fashion_mnist)\n",
    "\n",
    "DATASET_NAME = 'fashion_mnist'\n",
    "GAN_NAME = f'CGAN/{DATASET_NAME}'\n",
    "\n",
    "hash = train(g_model, d_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cda089-23fa-4dac-86bb-a836d5b01afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7fe15-afa3-43cd-aa3b-bffcbb1991e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283d970-9f4e-4d51-b44f-b7beb011c05f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
